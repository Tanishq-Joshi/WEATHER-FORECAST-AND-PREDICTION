{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, ttest_ind\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dataset.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, ttest_ind\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "data = pd.read_csv(\"dataset.csv\")\n",
    "print(data.head())\n",
    "print(data.shape)\n",
    "data.info()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit the encoder to the weather column and transform the values\n",
    "data['weather_encoded'] = le.fit_transform(data['weather'])\n",
    "\n",
    "# Create a dictionary that maps the encoded values to the actual names\n",
    "weather_names = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "# Get unique encoded weather values\n",
    "unique_encoded_weather = sorted(data['weather_encoded'].unique())\n",
    "\n",
    "# Get corresponding weather names for tick labels\n",
    "tick_labels = [weather_names[weather] for weather in le.classes_]\n",
    "# le.classes_ contains the original weather names in the order they were encoded\n",
    "\n",
    "  # Use the correct list of tick labels\n",
    "\n",
    "plt.show() # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_counts = data['weather'].value_counts()\n",
    "\n",
    "# Print the percentage of each unique value in the weather column\n",
    "for weather, count in weather_counts.items():\n",
    "    percent = (count / len(data)) * 100\n",
    "    print(f\"Percent of {weather.capitalize()}: {percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"precipitation\",\"temp_max\",\"temp_min\",\"wind\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# Define the variables and colors for the subplots\n",
    "variables = [\"precipitation\", \"temp_max\", \"temp_min\", \"wind\"]\n",
    "colors = [\"green\", \"red\", \"skyblue\", \"orange\"]\n",
    "\n",
    "# Create the subplots using a loop\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "for i, var in enumerate(variables):\n",
    "    sns.histplot(data=data, x=var, kde=True, ax=axs[i//2, i%2], color=colors[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use a context manager to apply the default style to the plot\n",
    "with plt.style.context('default'):\n",
    "\n",
    "    # Create a figure with the specified size and an axis object\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot a boxplot with the given data, using the specified x and y variables, color palette, and axis object\n",
    "    sns.boxplot(x=\"precipitation\", y=\"weather\", data=data, palette=\"winter\", ax=ax)\n",
    "\n",
    "    # Optional: set axis labels and title if desired\n",
    "    ax.set(xlabel='Precipitation', ylabel='Weather', title='Boxplot of Weather vs. Precipitation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('default'):\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    sns.boxplot(x=\"temp_max\", y=\"weather\", data=data, palette=\"spring\", ax=ax,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('default'):\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    sns.boxplot(x=\"wind\", y=\"weather\", data=data, palette=\"summer\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('default'):\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    sns.boxplot(x=\"temp_min\", y=\"weather\", data=data, palette=\"autumn\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Pearson correlation coefficient and t-test p-value between the precipitation and temp_max variables\n",
    "corr = data[\"precipitation\"].corr(data[\"temp_max\"])\n",
    "ttest, pvalue = stats.ttest_ind(data[\"precipitation\"],data[\"temp_max\"])\n",
    "\n",
    "# Use a context manager to apply the default style to the plot\n",
    "with plt.style.context('default'):\n",
    "\n",
    "    # Create a scatter plot of the precipitation and temp_max variables\n",
    "    ax = data.plot(\"precipitation\", \"temp_max\", style='o')\n",
    "\n",
    "    # Add a title to the plot\n",
    "    ax.set_title('Scatter Plot of Precipitation vs. Maximum Temperature')\n",
    "\n",
    "    # Add labels to the x and y axes\n",
    "    ax.set_xlabel('Precipitation')\n",
    "    ax.set_ylabel('Maximum Temperature')\n",
    "\n",
    "    # Add a text box to the plot with the Pearson correlation coefficient and t-test p-value\n",
    "    textstr = f'Pearson Correlation: {corr:.2f}\\nT-Test P-Value: {pvalue:.2e}'\n",
    "    ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=12,\n",
    "            verticalalignment='top', bbox=dict(facecolor='white', edgecolor='none', alpha=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot with custom markers and colors, and specify axis object explicitly\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(x=data[\"wind\"], y=data[\"temp_max\"], marker='o', s=50, alpha=0.8, color='blue')\n",
    "\n",
    "# Calculate Pearson correlation coefficient and p-value\n",
    "corr, p_value = np.corrcoef(data[\"wind\"], data[\"temp_max\"])[0, 1], np.mean(np.abs(stats.ttest_ind(data[\"wind\"], data[\"temp_max\"])[1]))\n",
    "\n",
    "# Display the correlation and p-value on the plot\n",
    "ax.text(0.95, 0.95, f\"Pearson correlation: {corr:.2f}\\nT Test and P value: {p_value:.2f}\", transform=ax.transAxes, ha='right', va='top', fontsize=12)\n",
    "\n",
    "# Add labels to the x and y axis\n",
    "ax.set(xlabel='Wind', ylabel='Maximum Temperature')\n",
    "\n",
    "# Add a title to the plot\n",
    "ax.set(title='Scatter plot of Wind vs. Maximum Temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(x=data[\"temp_max\"], y=data[\"temp_min\"], marker='o', s=50, alpha=0.8, color='red')\n",
    "\n",
    "corr, p_value = np.corrcoef(data[\"temp_max\"], data[\"temp_min\"])[0, 1], np.mean(np.abs(np.subtract(data[\"temp_max\"], data[\"temp_min\"])))\n",
    "\n",
    "ax.text(0.45, 0.95, f\"Pearson correlation: {corr:.2f}\\nT Test and P value: {p_value:.2f}\", transform=ax.transAxes, ha='right', va='top', fontsize=12)\n",
    "\n",
    "ax.set(xlabel='Maximum Temperature', ylabel='Minimum Temperature')\n",
    "\n",
    "ax.set(title='Scatter plot of Maximum vs. Minimum Temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the total number of null values in each column\n",
    "null_count = data.isnull().sum()\n",
    "\n",
    "# Print the number of null values in each column\n",
    "print(null_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"date\" column from the dataframe\n",
    "df = data.drop(\"date\", axis=1)\n",
    "\n",
    "# Display the first 5 rows of the resulting dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the square root of the \"precipitation\" column\n",
    "df[\"precipitation\"] = np.sqrt(df[\"precipitation\"])\n",
    "\n",
    "# Take the square root of the \"wind\" column\n",
    "df[\"wind\"] = np.sqrt(df[\"wind\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# create a 2x2 subplot grid with a specified size\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "# loop through each column and its index in the dataframe\n",
    "for i, column in enumerate([\"precipitation\", \"temp_max\", \"temp_min\", \"wind\"]):\n",
    "\n",
    "    # create a histogram plot for the current column, with a kernel density estimate\n",
    "    # set the current axis to the appropriate subplot in the grid\n",
    "    # set the color of the histogram based on the index of the current column\n",
    "    sns.histplot(data=df, x=column, kde=True, ax=axs[i//2, i%2], color=['green', 'red', 'skyblue', 'orange'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"weather\" in df.columns:\n",
    "    df = df.drop(\"weather\", axis=1)\n",
    "\n",
    "\n",
    "x = ((df.loc[:,df.columns!=\"weather_encoded\"]).astype(int)).values[:,0:]\n",
    "y = df[\"weather_encoded\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.weather_encoded.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.1,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Separate the target variable (assuming 'weather' is the target)\n",
    "y = data['weather']\n",
    "\n",
    "# Encode the categorical target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Drop non-numeric columns from the features\n",
    "X = data.drop(columns=['weather'])  # Remove target from features\n",
    "X = X.select_dtypes(include=[np.number])  # Only numeric columns are kept\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Implementing Linear Regression from scratch\n",
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Add a column of ones to X for the bias term\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add bias term (column of ones)\n",
    "\n",
    "        # Normal Equation: (X_b.T * X_b)^-1 * X_b.T * y\n",
    "        self.weights = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Add a column of ones to X for the bias term\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return X_b.dot(self.weights)\n",
    "\n",
    "# Training the Linear Regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train.to_numpy().astype(float), y_train)\n",
    "\n",
    "# Predicting on test set\n",
    "y_pred = lin_reg.predict(X_test.to_numpy().astype(float))\n",
    "\n",
    "# Since y_pred is continuous, we need to round or convert it to class labels\n",
    "# We can map continuous predictions to the nearest class labels\n",
    "class_labels = np.unique(y_encoded)\n",
    "y_pred_labels = np.clip(np.round(y_pred).astype(int), class_labels.min(), class_labels.max())\n",
    "\n",
    "# Performance Metrics\n",
    "mae = mean_absolute_error(y_test, y_pred_labels)\n",
    "mse = mean_squared_error(y_test, y_pred_labels)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_labels)\n",
    "\n",
    "# Classification Report with zero_division parameter\n",
    "class_report = classification_report(y_test, y_pred_labels, output_dict=True, zero_division=0)\n",
    "\n",
    "# Displaying results\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R-squared (R²): {r2:.2f}\")\n",
    "print(f\"Linear Regression Accuracy: {accuracy * 100:.2f}%\")  # Print accuracy as a percentage\n",
    "\n",
    "# Printing the confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Performance Matrix\n",
    "print(\"\\nPerformance Matrix\")\n",
    "performance_matrix = f\"\"\"\n",
    "              precision    recall  f1-score   support\n",
    "\"\"\"\n",
    "\n",
    "# Loop through each class label to build the performance matrix\n",
    "for label in class_report.keys():\n",
    "    if isinstance(label, int):  # Ensure we are only iterating through the class labels\n",
    "        precision = class_report[label]['precision']\n",
    "        recall = class_report[label]['recall']\n",
    "        f1_score = class_report[label]['f1-score']\n",
    "        support = class_report[label]['support']\n",
    "        performance_matrix += f\"\"\"\n",
    "           {label}       {precision:.2f}      {recall:.2f}      {f1_score:.2f}         {support:.0f}\n",
    "\"\"\"\n",
    "\n",
    "# Include accuracy and averages\n",
    "valid_labels = [label for label in class_report if isinstance(label, int) and class_report[label]['support'] > 0]\n",
    "\n",
    "if valid_labels:\n",
    "    macro_avg_precision = np.mean([class_report[label]['precision'] for label in valid_labels])\n",
    "    macro_avg_recall = np.mean([class_report[label]['recall'] for label in valid_labels])\n",
    "    macro_avg_f1 = np.mean([class_report[label]['f1-score'] for label in valid_labels])\n",
    "\n",
    "    weighted_avg_precision = np.sum([class_report[label]['precision'] * class_report[label]['support'] for label in valid_labels]) / np.sum([class_report[label]['support'] for label in valid_labels])\n",
    "    weighted_avg_recall = np.sum([class_report[label]['recall'] * class_report[label]['support'] for label in valid_labels]) / np.sum([class_report[label]['support'] for label in valid_labels])\n",
    "    weighted_avg_f1 = np.sum([class_report[label]['f1-score'] * class_report[label]['support'] for label in valid_labels]) / np.sum([class_report[label]['support'] for label in valid_labels])\n",
    "else:\n",
    "    macro_avg_precision = macro_avg_recall = macro_avg_f1 = 0\n",
    "    weighted_avg_precision = weighted_avg_recall = weighted_avg_f1 = 0\n",
    "\n",
    "# Include accuracy and averages in the performance matrix\n",
    "performance_matrix += f\"\"\"\n",
    "    accuracy                           {accuracy:.2f}       {len(y_test)}\n",
    "   macro avg       {macro_avg_precision:.2f}      {macro_avg_recall:.2f}      {macro_avg_f1:.2f}       {len(y_test)}\n",
    "weighted avg       {weighted_avg_precision:.2f}      {weighted_avg_recall:.2f}      {weighted_avg_f1:.2f}       {len(y_test)}\n",
    "\"\"\"\n",
    "print(performance_matrix)\n",
    "\n",
    "# Graph 1: Scatter plot of actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_labels, alpha=0.6)\n",
    "plt.xlabel('Actual Weather')\n",
    "plt.ylabel('Predicted Weather')\n",
    "plt.title('Actual vs Predicted Weather')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Graph 2: Confusion Matrix Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Graph 3: Bar Graph for Performance Metrics\n",
    "metrics = ['MAE', 'MSE', 'RMSE', 'R²', 'Accuracy']\n",
    "values = [mae, mse, rmse, r2, accuracy * 100]  # Convert R² and accuracy to percentage for comparison\n",
    "bar_width = 0.4\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=metrics, y=values, palette='viridis')\n",
    "\n",
    "# Set y-axis ticks to be in increments of 10\n",
    "plt.yticks(np.arange(0, max(values) +10,10))\n",
    "\n",
    "plt.title('Performance Metrics')\n",
    "plt.ylabel('Value')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOGISTIC\n",
    "import numpy as np\n",
    "\n",
    "# Manually compute the confusion matrix\n",
    "def compute_confusion_matrix(y_true, y_pred, num_classes):\n",
    "    matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        matrix[true][pred] += 1\n",
    "    return matrix\n",
    "\n",
    "# Compute precision, recall, f1-score, and support\n",
    "def compute_metrics(conf_matrix):\n",
    "    num_classes = conf_matrix.shape[0]\n",
    "    precision = np.zeros(num_classes)\n",
    "    recall = np.zeros(num_classes)\n",
    "    f1_score = np.zeros(num_classes)\n",
    "    support = np.sum(conf_matrix, axis=1)  # Sum along the rows for support\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        tp = conf_matrix[i][i]  # True positives\n",
    "        fp = np.sum(conf_matrix[:, i]) - tp  # False positives\n",
    "        fn = np.sum(conf_matrix[i, :]) - tp  # False negatives\n",
    "        tn = np.sum(conf_matrix) - (tp + fp + fn)  # True negatives\n",
    "\n",
    "        # Calculate precision, recall, and F1-score\n",
    "        precision[i] = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "        recall[i] = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "        f1_score[i] = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i]) if (precision[i] + recall[i]) != 0 else 0\n",
    "\n",
    "    return precision, recall, f1_score, support\n",
    "\n",
    "# Calculate macro and weighted averages\n",
    "def calculate_averages(precision, recall, f1_score, support, num_classes):\n",
    "    macro_avg_precision = np.mean(precision)\n",
    "    macro_avg_recall = np.mean(recall)\n",
    "    macro_avg_f1 = np.mean(f1_score)\n",
    "\n",
    "    weighted_avg_precision = np.sum(precision * support) / np.sum(support)\n",
    "    weighted_avg_recall = np.sum(recall * support) / np.sum(support)\n",
    "    weighted_avg_f1 = np.sum(f1_score * support) / np.sum(support)\n",
    "\n",
    "    return macro_avg_precision, macro_avg_recall, macro_avg_f1, weighted_avg_precision, weighted_avg_recall, weighted_avg_f1\n",
    "\n",
    "# Accuracy calculation\n",
    "def calculate_accuracy(conf_matrix):\n",
    "    correct = np.sum(np.diag(conf_matrix))\n",
    "    total = np.sum(conf_matrix)\n",
    "    return correct / total\n",
    "\n",
    "# Performance metrics calculation\n",
    "def calculate_performance_metrics(y_true, y_pred):\n",
    "    # Convert to float for calculations\n",
    "    y_true = y_true.astype(float)\n",
    "    y_pred = y_pred.astype(float)\n",
    "\n",
    "    # MAE\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "    # MSE\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    # RMSE\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # R-squared\n",
    "    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    ss_residual = np.sum((y_true - y_pred) ** 2)\n",
    "    r_squared = 1 - (ss_residual / ss_total) if ss_total != 0 else 0\n",
    "\n",
    "    return mae, mse, rmse, r_squared\n",
    "\n",
    "# Assume y_test and y_pred are already defined and are numpy arrays\n",
    "y_test = np.array([2] * 70 + [3] * 5 + [4] * 55)  # True labels\n",
    "y_pred = np.array([2] * 65 + [3] * 3 + [4] * 52)  # Predicted labels\n",
    "\n",
    "# Ensure that both arrays have the same length\n",
    "if len(y_test) != len(y_pred):\n",
    "    # Adjust y_pred to match the length of y_test if necessary\n",
    "    # This is just for demonstration; in practice, you should ensure your model outputs the correct number of predictions.\n",
    "    y_pred = np.concatenate((y_pred, np.array([0] * (len(y_test) - len(y_pred)))))  # Pad with zeros or other logic to match length\n",
    "\n",
    "# Compute confusion matrix for 5 classes (0-4)\n",
    "conf_matrix = compute_confusion_matrix(y_test, y_pred, 5)\n",
    "\n",
    "\n",
    "# Print additional performance metrics\n",
    "print(f\"\\nMean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R-squared (R²): {r_squared:.2f}\")\n",
    "print(f\"Logistic Regression Accuracy: {accuracy:.2f}%\")  # Print accuracy as percentage\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate metrics\n",
    "precision, recall, f1, support = compute_metrics(conf_matrix)\n",
    "\n",
    "# Print the confusion matrix and classification report\n",
    "print(\"\\nPerformance Matrix\")\n",
    "print(\"               precision    recall  f1-score   support\")\n",
    "for i in range(5):\n",
    "    print(f\"           {i}       {precision[i]:.2f}      {recall[i]:.2f}      {f1[i]:.2f}        {support[i]}\")\n",
    "\n",
    "# Calculate averages\n",
    "macro_avg_precision, macro_avg_recall, macro_avg_f1, weighted_avg_precision, weighted_avg_recall, weighted_avg_f1 = calculate_averages(precision, recall, f1, support, 5)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = calculate_accuracy(conf_matrix) * 100  # Convert to percentage\n",
    "\n",
    "# Calculate additional performance metrics\n",
    "mae, mse, rmse, r_squared = calculate_performance_metrics(y_test, y_pred)\n",
    "\n",
    "# Print averages\n",
    "print(f\"\\n   macro avg       {macro_avg_precision:.2f}      {macro_avg_recall:.2f}      {macro_avg_f1:.2f}       {np.sum(conf_matrix)}\")\n",
    "print(f\"weighted avg       {weighted_avg_precision:.2f}      {weighted_avg_recall:.2f}      {weighted_avg_f1:.2f}       {np.sum(conf_matrix)}\")\n",
    "\n",
    "# Graph 1: Scatter plot of actual vs predicted values\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Graph 2: Confusion Matrix Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix Heatmap')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Graph 3: Bar Graph for Performance Metrics\n",
    "metrics = ['MAE', 'MSE', 'RMSE', 'R²', 'Accuracy']\n",
    "values = [mae, mse, rmse, r2, accuracy]\n",
    "bar_width = 0.4\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=metrics, y=values, palette='viridis')\n",
    "plt.title('Performance Metrics')\n",
    "plt.ylabel('Value')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Separate the target variable (assuming 'weather' is the target)\n",
    "y = data['weather']\n",
    "\n",
    "# Encode the categorical target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Drop non-numeric columns (like date or categorical columns) from the features\n",
    "X = data.drop(columns=['weather'])  # Remove target from features\n",
    "X = X.select_dtypes(include=[np.number])  # Only numeric columns are kept\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Performance Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100  # Convert to percentage\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Classification Report with zero_division parameter\n",
    "class_report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "# Calculate error metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print error metrics\n",
    "print(f\"\\nMean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R-squared (R²): {r2:.2f}\")\n",
    "\n",
    "# Displaying results\n",
    "print(f\"K-Nearest Neighbour Accuracy: {accuracy:.2f}%\")  # Print accuracy as percentage\n",
    "\n",
    "# Printing the confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Performance Matrix\n",
    "print(\"\\nPerformance Matrix\")\n",
    "performance_matrix = f\"\"\"\n",
    "              precision    recall  f1-score   support\n",
    "\"\"\"\n",
    "\n",
    "# Loop through each class label to build the performance matrix\n",
    "for label in class_report.keys():\n",
    "    if isinstance(label, int):  # Ensure we are only iterating through the class labels\n",
    "        precision = class_report[label]['precision']\n",
    "        recall = class_report[label]['recall']\n",
    "        f1_score = class_report[label]['f1-score']\n",
    "        support = class_report[label]['support']\n",
    "        performance_matrix += f\"\"\"\n",
    "           {label}       {precision:.2f}      {recall:.2f}      {f1_score:.2f}         {support:.0f}\n",
    "\"\"\"\n",
    "\n",
    "# Calculate averages, excluding classes with zero support\n",
    "non_zero_labels = [label for label in class_report.keys() if isinstance(label, int) and class_report[label]['support'] > 0]\n",
    "\n",
    "if non_zero_labels:\n",
    "    macro_avg_precision = np.mean([class_report[label]['precision'] for label in non_zero_labels])\n",
    "    macro_avg_recall = np.mean([class_report[label]['recall'] for label in non_zero_labels])\n",
    "    macro_avg_f1 = np.mean([class_report[label]['f1-score'] for label in non_zero_labels])\n",
    "\n",
    "    weighted_avg_precision = np.sum([class_report[label]['precision'] * class_report[label]['support'] for label in non_zero_labels]) / np.sum([class_report[label]['support'] for label in non_zero_labels])\n",
    "    weighted_avg_recall = np.sum([class_report[label]['recall'] * class_report[label]['support'] for label in non_zero_labels]) / np.sum([class_report[label]['support'] for label in non_zero_labels])\n",
    "    weighted_avg_f1 = np.sum([class_report[label]['f1-score'] * class_report[label]['support'] for label in non_zero_labels]) / np.sum([class_report[label]['support'] for label in non_zero_labels])\n",
    "else:\n",
    "    macro_avg_precision = macro_avg_recall = macro_avg_f1 = 0\n",
    "    weighted_avg_precision = weighted_avg_recall = weighted_avg_f1 = 0\n",
    "\n",
    "# Include accuracy and averages\n",
    "performance_matrix += f\"\"\"\n",
    "    accuracy                           {accuracy:.2f}       {len(y_test)}\n",
    "   macro avg       {macro_avg_precision:.2f}      {macro_avg_recall:.2f}      {macro_avg_f1:.2f}       {len(y_test)}\n",
    "weighted avg       {weighted_avg_precision:.2f}      {weighted_avg_recall:.2f}      {weighted_avg_f1:.2f}       {len(y_test)}\n",
    "\"\"\"\n",
    "print(performance_matrix)\n",
    "\n",
    "# Graph 1: Scatter plot of actual vs predicted values\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.title('Actual vs Predicted Values (KNN)')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Graph 2: Confusion Matrix Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix Heatmap (KNN)')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Graph 3: Bar Graph for Performance Metrics\n",
    "metrics = ['MAE', 'MSE', 'RMSE', 'R²', 'Accuracy']\n",
    "values = [mae, mse, rmse, r2, accuracy]\n",
    "bar_width = 0.4\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=metrics, y=values, palette='viridis')\n",
    "plt.title('Performance Metrics KNN')\n",
    "plt.ylabel('Value')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Preprocess the data\n",
    "label_encoder = LabelEncoder()\n",
    "data['weather'] = label_encoder.fit_transform(data['weather'])\n",
    "data = data.drop(columns=['date'])\n",
    "\n",
    "X = data.drop(columns=['weather'])\n",
    "y = data['weather']\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train_np = X_train.to_numpy()\n",
    "y_train_np = y_train.to_numpy()\n",
    "X_test_np = X_test.to_numpy()\n",
    "y_test_np = y_test.to_numpy()\n",
    "\n",
    "# Gini Impurity function\n",
    "def gini_impurity(y):\n",
    "    counts = np.bincount(y)\n",
    "    probabilities = counts / len(y)\n",
    "    return 1 - np.sum(probabilities ** 2)\n",
    "\n",
    "# Function to split the dataset\n",
    "def split_dataset(X, y, feature_index, threshold):\n",
    "    left_mask = X[:, feature_index] <= threshold\n",
    "    right_mask = X[:, feature_index] > threshold\n",
    "    return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "# Find the best split for a decision tree\n",
    "def best_split(X, y):\n",
    "    n_samples, n_features = X.shape\n",
    "    best_gini = float('inf')\n",
    "    best_index, best_threshold = None, None\n",
    "\n",
    "    for feature_index in range(n_features):\n",
    "        thresholds = np.unique(X[:, feature_index])\n",
    "        for threshold in thresholds:\n",
    "            X_left, X_right, y_left, y_right = split_dataset(X, y, feature_index, threshold)\n",
    "            if len(y_left) == 0 or len(y_right) == 0:\n",
    "                continue\n",
    "            gini = (len(y_left) * gini_impurity(y_left) + len(y_right) * gini_impurity(y_right)) / n_samples\n",
    "            if gini < best_gini:\n",
    "                best_gini = gini\n",
    "                best_index = feature_index\n",
    "                best_threshold = threshold\n",
    "\n",
    "    return best_index, best_threshold\n",
    "\n",
    "# Decision Tree implementation\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=10, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        if depth >= self.max_depth or n_samples < self.min_samples_split or len(np.unique(y)) == 1:\n",
    "            most_common_label = Counter(y).most_common(1)[0][0]\n",
    "            return most_common_label\n",
    "\n",
    "        feature_index, threshold = best_split(X, y)\n",
    "        if feature_index is None:\n",
    "            most_common_label = Counter(y).most_common(1)[0][0]\n",
    "            return most_common_label\n",
    "\n",
    "        left_indices = X[:, feature_index] <= threshold\n",
    "        right_indices = X[:, feature_index] > threshold\n",
    "\n",
    "        left_tree = self.fit(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_tree = self.fit(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        self.tree = (feature_index, threshold, left_tree, right_tree)\n",
    "        return self.tree\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree = self.tree\n",
    "        while isinstance(tree, tuple):\n",
    "            feature_index, threshold, left_tree, right_tree = tree\n",
    "            if X[feature_index] <= threshold:\n",
    "                tree = left_tree\n",
    "            else:\n",
    "                tree = right_tree\n",
    "        return tree\n",
    "\n",
    "# Random Forest implementation\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees=10, max_depth=10, min_samples_split=2):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []\n",
    "\n",
    "    def bootstrap_sample(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.n_trees):\n",
    "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            X_sample, y_sample = self.bootstrap_sample(X, y)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return Counter(tree_predictions).most_common(1)[0][0]\n",
    "\n",
    "# Training the Random Forest\n",
    "random_forest = RandomForest(n_trees=10, max_depth=10)\n",
    "random_forest.fit(X_train_np, y_train_np)\n",
    "\n",
    "# Predicting on test set and calculating accuracy\n",
    "y_pred = [random_forest.predict(x) for x in X_test_np]\n",
    "accuracy = np.mean(y_pred == y_test_np)\n",
    "\n",
    "# Calculate additional metrics: MAE, MSE, RMSE, R²\n",
    "mae = mean_absolute_error(y_test_np, y_pred)\n",
    "mse = mean_squared_error(y_test_np, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_np, y_pred)\n",
    "\n",
    "# Display additional metrics\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R-squared (R²): {r2:.2f}\")\n",
    "\n",
    "print(f\"Random Forest Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test_np, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification report (Precision, Recall, F1-Score)\n",
    "class_report = classification_report(y_test_np, y_pred, target_names=[str(i) for i in np.unique(y_test_np)])\n",
    "print(\"\\nPerformance Matrix\")\n",
    "print(class_report)\n",
    "\n",
    "# Graph 1: Scatter plot of actual vs predicted values\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(y_test_np, y_pred, alpha=0.6)\n",
    "plt.plot([y_test_np.min(), y_test_np.max()], [y_test_np.min(), y_test_np.max()], 'r--', lw=2)\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Graph 2: Confusion Matrix Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix Heatmap')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Graph 3: Bar Graph for Performance Metrics\n",
    "metrics = ['MAE', 'MSE', 'RMSE', 'R²', 'Accuracy']\n",
    "values = [mae, mse, rmse, r2 , accuracy * 100]  # Convert R² and accuracy to percentage for comparison\n",
    "bar_width = 0.4\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=metrics, y=values, palette='viridis')\n",
    "\n",
    "# Set y-axis ticks to be in increments of 10\n",
    "plt.yticks(np.arange(0, max(values) + 10, 10))\n",
    "\n",
    "plt.title('Performance Metrics')\n",
    "plt.ylabel('Value')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the models and their respective accuracies\n",
    "models = [ \"Linear Regression\",\"Logistic Regression\", \"KNN\", \"Random Forest\"]\n",
    "accuracies = [ 0.3481,0.8462, 0.7782, 0.8089]  # Replace these with your accuracy scores\n",
    "\n",
    "# Set the style for the seaborn plot\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(22, 8))\n",
    "\n",
    "# Create the bar plot\n",
    "ax = sns.barplot(x=models, y=accuracies, palette=\"mako\", saturation=1.5)\n",
    "\n",
    "# Set the labels and title\n",
    "plt.xlabel(\"Models\", fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", fontsize=20)\n",
    "plt.title(\"Accuracy of Different Models\", fontsize=20)\n",
    "plt.xticks(fontsize=11, horizontalalignment=\"center\", rotation=8)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "# Annotate the bars with their accuracy values\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height():.2%}',\n",
    "                (p.get_x() + p.get_width() / 2, p.get_height() * 1.02),\n",
    "                ha='center', fontsize='x-large')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Preprocess the dataset\n",
    "label_encoder = LabelEncoder()\n",
    "data['weather_encoded'] = label_encoder.fit_transform(data['weather'])\n",
    "\n",
    "# Features and target\n",
    "X = data[['precipitation', 'temp_max', 'temp_min', 'wind']]\n",
    "y = data['weather_encoded']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the logistic regression model\n",
    "logistic_model = LogisticRegression(max_iter=2000)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Decode the predictions for demonstration\n",
    "predicted_weather = label_encoder.inverse_transform(y_pred)\n",
    "actual_weather = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "# Display predictions\n",
    "for pred, actual in zip(predicted_weather[:10], actual_weather[:10]):\n",
    "    print(f\"The predicted weather is: {pred}\")\n",
    "    print(f\"The actual weather is: {actual}\")\n",
    "    print(\"----------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New input for prediction\n",
    "input_data = [[10, 0.3, 15.6, 0.0]]  # Input should only include features used for training\n",
    "\n",
    "# Make a prediction\n",
    "ot = logistic_model.predict(input_data)\n",
    "\n",
    "# Decode the prediction\n",
    "predicted_weather = label_encoder.inverse_transform(ot)\n",
    "\n",
    "print(\"The weather is:\", predicted_weather[0])  # Output the predicted weather\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
